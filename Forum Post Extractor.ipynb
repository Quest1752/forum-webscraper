{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bdf5532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, ttk\n",
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from docx import Document\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Setting up basic logging configuration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class ScraperApp:\n",
    "    def __init__(self, root):\n",
    "        # Initialize the main application window\n",
    "        self.root = root\n",
    "        self.root.title(\"Web Scraper\")\n",
    "        self.create_widgets()  # Call the method to create UI elements\n",
    "\n",
    "    def create_widgets(self):\n",
    "        # Creating UI elements for user input and actions\n",
    "        tk.Label(self.root, text=\"Base URL:\").grid(row=0, column=0, padx=10, pady=5)\n",
    "        self.base_url_entry = tk.Entry(self.root, width=50)\n",
    "        self.base_url_entry.grid(row=0, column=1, padx=10, pady=5)\n",
    "        \n",
    "        tk.Label(self.root, text=\"Page Range Start:\").grid(row=1, column=0, padx=10, pady=5)\n",
    "        self.page_start_entry = tk.Entry(self.root)\n",
    "        self.page_start_entry.grid(row=1, column=1, padx=10, pady=5)\n",
    "\n",
    "        tk.Label(self.root, text=\"Page Range End:\").grid(row=2, column=0, padx=10, pady=5)\n",
    "        self.page_end_entry = tk.Entry(self.root)\n",
    "        self.page_end_entry.grid(row=2, column=1, padx=10, pady=5)\n",
    "\n",
    "        tk.Label(self.root, text=\"Author Filter (optional):\").grid(row=3, column=0, padx=10, pady=5)\n",
    "        self.author_filter_entry = tk.Entry(self.root)\n",
    "        self.author_filter_entry.grid(row=3, column=1, padx=10, pady=5)\n",
    "\n",
    "        # New UI element for relevant names filtering\n",
    "        tk.Label(self.root, text=\"Relevant Names (comma separated, optional):\").grid(row=4, column=0, padx=10, pady=5)\n",
    "        self.relevant_names_entry = tk.Entry(self.root)\n",
    "        self.relevant_names_entry.grid(row=4, column=1, padx=10, pady=5)\n",
    "\n",
    "        # New UI element for setting a word limit per document\n",
    "        tk.Label(self.root, text=\"Word Limit Per Document (optional):\").grid(row=5, column=0, padx=10, pady=5)\n",
    "        self.word_limit_entry = tk.Entry(self.root)\n",
    "        self.word_limit_entry.grid(row=5, column=1, padx=10, pady=5)\n",
    "\n",
    "        # Checkbox for threadmarked articles only\n",
    "        self.threadmarked_only_var = tk.IntVar()\n",
    "        self.threadmarked_only_check = tk.Checkbutton(self.root, text=\"Threadmarked Articles Only\", variable=self.threadmarked_only_var)\n",
    "        self.threadmarked_only_check.grid(row=6, columnspan=2, pady=5)\n",
    "\n",
    "        # Checkbox for including threadmark labels\n",
    "        self.include_threadmark_var = tk.IntVar()\n",
    "        self.include_threadmark_check = tk.Checkbutton(self.root, text=\"Include Threadmark Labels\", variable=self.include_threadmark_var)\n",
    "        self.include_threadmark_check.grid(row=7, columnspan=2, pady=5)\n",
    "\n",
    "        # Checkbox for including author tags\n",
    "        self.include_author_var = tk.IntVar()\n",
    "        self.include_author_check = tk.Checkbutton(self.root, text=\"Include Author Tags\", variable=self.include_author_var)\n",
    "        self.include_author_check.grid(row=8, columnspan=2, pady=5)\n",
    "\n",
    "        # Checkbox for including separator lines\n",
    "        self.include_separator_var = tk.IntVar()\n",
    "        self.include_separator_check = tk.Checkbutton(self.root, text=\"Include Separator Lines\", variable=self.include_separator_var)\n",
    "        self.include_separator_check.grid(row=9, columnspan=2, pady=5)\n",
    "\n",
    "        # Button to start the scraper\n",
    "        tk.Button(self.root, text=\"Run Scraper\", command=self.start_scraping_thread).grid(row=10, columnspan=2, pady=10)\n",
    "\n",
    "        # Progress bar to show the progress of the scraping process\n",
    "        self.progress = ttk.Progressbar(self.root, orient=\"horizontal\", length=400, mode=\"determinate\")\n",
    "        self.progress.grid(row=11, columnspan=2, pady=5)\n",
    "\n",
    "        # Text box to display logs and messages to the user\n",
    "        self.log_text = tk.Text(self.root, height=10, state='disabled', wrap='word')\n",
    "        self.log_text.grid(row=12, columnspan=2, padx=10, pady=5)\n",
    "\n",
    "    def log_message(self, message):\n",
    "        # Method to log messages in the text box\n",
    "        self.log_text.config(state='normal')\n",
    "        self.log_text.insert(tk.END, message + '\\n')\n",
    "        self.log_text.see(tk.END)\n",
    "        self.log_text.config(state='disabled')\n",
    "\n",
    "    def start_scraping_thread(self):\n",
    "        # Start the scraping process in a separate thread to avoid freezing the UI\n",
    "        thread = threading.Thread(target=self.run_scraper)\n",
    "        thread.start()\n",
    "\n",
    "    def run_scraper(self):\n",
    "        # Main method to handle the scraping process\n",
    "\n",
    "        # Get the base URL and remove any trailing slashes\n",
    "        base_url = self.base_url_entry.get().rstrip('/')\n",
    "        if not base_url.startswith(\"http\"):\n",
    "            messagebox.showerror(\"Invalid input\", \"Base URL must be a valid URL starting with http or https.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Get the page range from the user input\n",
    "            page_start = int(self.page_start_entry.get())\n",
    "            page_end = int(self.page_end_entry.get())\n",
    "        except ValueError:\n",
    "            # Show an error if the page range is not a valid integer\n",
    "            messagebox.showerror(\"Invalid input\", \"Page range must be integers.\")\n",
    "            return\n",
    "\n",
    "        # Ask the user to choose a directory to save the output files\n",
    "        output_dir = filedialog.askdirectory()\n",
    "        if not output_dir:\n",
    "            return\n",
    "\n",
    "        # Process the relevant names input into a list, if provided\n",
    "        relevant_names_input = self.relevant_names_entry.get()\n",
    "        relevant_names = [name.strip() for name in relevant_names_input.split(\",\")] if relevant_names_input else []\n",
    "        relevant_names_lower = [name.lower() for name in relevant_names]\n",
    "        documents = {name: [Document()] for name in relevant_names} if relevant_names else {\"default\": [Document()]}\n",
    "\n",
    "        # Optional author filter\n",
    "        author_filter = self.author_filter_entry.get() or None\n",
    "\n",
    "        # Optional word limit\n",
    "        try:\n",
    "            word_limit = int(self.word_limit_entry.get()) if self.word_limit_entry.get() else None\n",
    "        except ValueError:\n",
    "            messagebox.showerror(\"Invalid input\", \"Word limit must be an integer.\")\n",
    "            return\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Use ThreadPoolExecutor to handle multiple page requests concurrently\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            future_to_page = {\n",
    "                executor.submit(\n",
    "                    self.scrape_page, base_url, page_num, author_filter, relevant_names_lower, documents, word_limit\n",
    "                ): page_num for page_num in range(page_start, page_end + 1)\n",
    "            }\n",
    "\n",
    "            # Process the results as they are completed\n",
    "            for future in as_completed(future_to_page):\n",
    "                page_num = future_to_page[future]\n",
    "                try:\n",
    "                    future.result()  # Retrieve the result of the scraping\n",
    "                except Exception as e:\n",
    "                    # Log any errors encountered during the scraping process\n",
    "                    self.log_message(f\"Error processing page {page_num}: {e}\")\n",
    "                # Update the progress bar\n",
    "                self.progress[\"value\"] += 1\n",
    "                self.root.update_idletasks()\n",
    "\n",
    "        # Save each document in the output directory\n",
    "        if relevant_names:\n",
    "            for name, doc_list in documents.items():\n",
    "                for i, doc in enumerate(doc_list):\n",
    "                    save_path = os.path.join(output_dir, f\"{name}_{i+1}.docx\")\n",
    "                    doc.save(save_path)\n",
    "                    self.log_message(f\"Saved document as '{save_path}'.\")\n",
    "        else:\n",
    "            # If no relevant names were provided, save a single document\n",
    "            save_path = os.path.join(output_dir, \"scraped_content.docx\")\n",
    "            documents[\"default\"][0].save(save_path)\n",
    "            self.log_message(f\"Saved document as '{save_path}'.\")\n",
    "\n",
    "        # Notify the user that the scraping is complete\n",
    "        messagebox.showinfo(\"Success\", \"Scraping completed successfully!\")\n",
    "\n",
    "    def scrape_page(self, base_url, page_num, author_filter, relevant_names_lower, documents, word_limit):\n",
    "        # Method to scrape a single page of content\n",
    "\n",
    "        # Set up headers for the HTTP request\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        url = f\"{base_url}/page-{page_num}\"  # Construct the URL for the current page\n",
    "        self.log_message(f\"Fetching page {page_num}...\")\n",
    "\n",
    "        try:\n",
    "            # Send the GET request to fetch the page content\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')  # Parse the HTML content\n",
    "\n",
    "            # Filter for threadmarked articles if the option is selected\n",
    "            if self.threadmarked_only_var.get():\n",
    "                articles = soup.find_all(\"article\", attrs={\"class\": re.compile(r\".*\\bhasThreadmark\\b.*\")})\n",
    "            else:\n",
    "                articles = soup.find_all(\"article\")\n",
    "\n",
    "            for article in articles:\n",
    "                article_text = article.get_text().lower()\n",
    "\n",
    "                # Check author filter\n",
    "                if author_filter and author_filter.lower() not in article_text:\n",
    "                    continue\n",
    "\n",
    "                if relevant_names_lower:\n",
    "                    for name, name_lower in zip(documents.keys(), relevant_names_lower):\n",
    "                        if name_lower in article_text:\n",
    "                            self.process_article(article, name, documents, word_limit)\n",
    "                            break\n",
    "                else:\n",
    "                    self.process_article(article, \"default\", documents, word_limit)\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            self.log_message(f\"Failed to fetch {url}: {e}\")\n",
    "\n",
    "    def process_article(self, article, name, documents, word_limit):\n",
    "        # Method to process and add an article's content to a document\n",
    "        bb_wrapper = article.find(\"div\", class_=\"bbWrapper\")\n",
    "        if bb_wrapper:\n",
    "            current_doc = documents[name][-1]\n",
    "            current_word_count = sum(len(p.text.split()) for p in current_doc.paragraphs)\n",
    "\n",
    "            if word_limit and current_word_count + len(bb_wrapper.get_text().split()) > word_limit:\n",
    "                new_doc = Document()\n",
    "                documents[name].append(new_doc)\n",
    "                current_doc = new_doc\n",
    "\n",
    "            # Include author tag if selected\n",
    "            if self.include_author_var.get():\n",
    "                data_author = article.get(\"data-author\", \"Unknown Author\")\n",
    "                current_doc.add_paragraph(f\"Author: {data_author}\")\n",
    "                self.log_message(f\"Added author: {data_author}\")\n",
    "\n",
    "            # Include threadmark label if selected\n",
    "            if self.include_threadmark_var.get():\n",
    "                threadmark_label = article.find(\"span\", class_=\"threadmarkLabel\")\n",
    "                if threadmark_label:\n",
    "                    threadmark_text = threadmark_label.get_text()\n",
    "                    current_doc.add_paragraph(f\"Threadmark: {threadmark_text}\")\n",
    "                    self.log_message(f\"Added threadmark label: {threadmark_text}\")\n",
    "\n",
    "            # Add the article's main content\n",
    "            bb_wrapper_text = bb_wrapper.get_text()\n",
    "            current_doc.add_paragraph(bb_wrapper_text)\n",
    "\n",
    "            # Include separator line if selected\n",
    "            if self.include_separator_var.get():\n",
    "                current_doc.add_paragraph(\"--------------------\")\n",
    "                self.log_message(\"Added separator line.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the main application loop\n",
    "    root = tk.Tk()\n",
    "    app = ScraperApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "934c0f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.10.2-cp39-cp39-win_amd64.whl (378 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-win_amd64.whl (50 kB)\n",
      "Collecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from aiohttp) (21.2.0)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp39-cp39-win_amd64.whl (76 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.3.5-py3-none-any.whl (12 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: idna>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp) (3.2)\n",
      "Installing collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, aiohappyeyeballs, aiohttp\n",
      "Successfully installed aiohappyeyeballs-2.3.5 aiohttp-3.10.2 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 multidict-6.0.5 yarl-1.9.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install aiohttp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bba776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
